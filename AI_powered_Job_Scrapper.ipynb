{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NzKrOPjC8REf",
        "outputId": "c7622152-aa0e-4664-db81-fdab355a1f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio # Install gradio for web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Np78JOfP8xk6",
        "outputId": "a1cae073-870b-46d7-e70c-44a1e8b93373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.40.30-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from flair) (0.34.4)\n",
            "Collecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from flair) (5.4.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.12/dist-packages (from flair) (10.8.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.11-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from flair) (2.9.0.post0)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from flair) (1.6.1)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.12/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.12/dist-packages (from flair) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.12/dist-packages (from flair) (4.67.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.56.1)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.41.0,>=1.40.30 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.40.30-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2.13->flair) (1.17.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair) (2.32.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair) (1.1.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair) (3.2.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.6.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.5)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.30->boto3>=1.20.27->flair) (2.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.1->flair) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.10.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.8)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.40.30-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.11-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.40.30-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=e6f0499c63ca147930d9c8159d4f11fc2df6d95155e77e1d5a58e636703e8027\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=eacf1e51c16618c7f9884a804ef65a78dfc722e36e0422948e490551b22b481f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/2d/de/37058114a8f07cfec75747cb46b864bc5c71b0e9e0e4cd0acd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=039468dfb1545862235e0dc35a2ce93bca5726de9c48c33417de40234e100c6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=a69715bfbc6d625f5dd1b03b1bf717784a21669b96edfabd18b6e023bc2148be\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=3447bfd2cdb77048a5ebd4e18181426aad1ddf93d30c1ddf67722e173c71345a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=6e66e213f7cadee958d6faa39d311e1d82443cc44cf193a8ed9d1a00133f0ea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, pptree, docopt, segtok, langdetect, jsonlines, jmespath, intervaltree, ftfy, deprecated, conllu, wikipedia-api, botocore, bioc, s3transfer, mpld3, pytorch-revgrad, boto3, transformer-smaller-training-vocab, flair\n",
            "Successfully installed bioc-2.1 boto3-1.40.30 botocore-1.40.30 conllu-4.5.3 deprecated-1.2.18 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.11 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.14.0 segtok-1.5.11 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.2 wikipedia-api-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flair # Flair for AIn skill detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fR0h8fq49lIh"
      },
      "outputs": [],
      "source": [
        "import gradio as gr # gradio to build the web app\n",
        "import requests # to fetch the web pages\n",
        "from bs4 import BeautifulSoup # this library is to parse html\n",
        "import pandas as pd\n",
        "from datetime import datetime # to add the dates\n",
        "import time # import time for adding delays in requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9KtnW-QSCvS6"
      },
      "outputs": [],
      "source": [
        "#import flair tools for AI skill detection\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "import threading # for smooth background tasks\n",
        "\n",
        "import os # to save the files\n",
        "import random # for random delayes\n",
        "\n",
        "# import retry tools to handle web errors\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0_6VAtAScq"
      },
      "source": [
        "Load AI Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "17259ae004d443e8b21c4bc0ce5e2e6e",
            "d007aedc45a149b889545f1b47d8208f",
            "4df204eac5e4432a956a673f5100262f",
            "94ac2c7c1a5e41f6a78ebf4296d65cd9",
            "e149aab73f9f40ea8ef7553e43838f02",
            "7e01df3e79ee4b828774f7f897cddc88",
            "2a4ebb5d94ba423e81c7f139a357972b",
            "af16c9524c324050bab76847cac365e3",
            "df19849846ea44b8a5da8392c6708b14",
            "fe7ccd4facd94b85becf3f55303a0d1a",
            "bf930c21b2314401b10ef686b6cb7334"
          ]
        },
        "id": "4LzTJqeDAWMF",
        "outputId": "ca6c2fe0-5e49-404f-c0bf-663e958b3bb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17259ae004d443e8b21c4bc0ce5e2e6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/169M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-13 11:31:16,248 SequenceTagger predicts: Dictionary with 7 tags: O, S-SKILL, B-SKILL, E-SKILL, I-SKILL, <START>, <STOP>\n"
          ]
        }
      ],
      "source": [
        "# load flair's skill detection model\n",
        "# flair_model = SequenceTagger.load(\"flair/ner-english-ontonotes-fast\")\n",
        "flair_model = SequenceTagger.load(\"kaliani/flair-ner-skill\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvm5HmAFEbhI"
      },
      "source": [
        "*Define filter Mappings*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PLGh0G9gEv01"
      },
      "outputs": [],
      "source": [
        "# Map experinc levels to LinkedIn URL codes\n",
        "\n",
        "experience_level_mapping ={\n",
        "    \"Internship\":\"f_E=1\",\n",
        "    \"Entry Level\":\"f_E=2\",\n",
        "    \"Associate\":\"f_E=3\",\n",
        "    \"Mid-Senior Level\":\"f_E=4\",\n",
        "}\n",
        "\n",
        "# Map work type to linkedIn URL codes\n",
        "work_type_mapping = {\n",
        "    \"On-site\":\"f_WT=1\",\n",
        "    \"Hybrid\":\"f_WT=2\",\n",
        "    \"Remote\":\"f_WT=3\"\n",
        "}\n",
        "\n",
        "#Map time filters to LinkedIn URL codes\n",
        "time_filter_mapping = {\n",
        "    \"Past Day\":\"f_TPR=r86400\",\n",
        "    \"Past Week\":\"f_TPR=r604800\",\n",
        "    \"Past Month\":\"f_TPR=r2592000\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JP7BH4uRK5LD"
      },
      "outputs": [],
      "source": [
        "Description = \"\"\"We are hiring - Python Modeling Developer\n",
        "\n",
        "\n",
        "\n",
        "About the Company\n",
        "\n",
        "\n",
        "\n",
        "Our Client is a global technology company focused on providing software solutions for the automotive and mobility industry. The company is headquartered in Pune, India, and operates in over 25 countries with delivery centers in Europe, the USA, Japan, China, and India.\n",
        "\n",
        "\n",
        "About the Role\n",
        "\n",
        "\n",
        "\n",
        "This role involves maintaining and developing features for the Adoption Curves Model, requiring strong Python modeling skills and the ability to work with various data management tools.\n",
        "\n",
        "\n",
        "Responsibilities\n",
        "\n",
        "\n",
        "\n",
        "Python Modeling (core): Maintain and develop features for Adoption Curves Model.\n",
        "Understand how to work with and add features to a Python calculation model.\n",
        "Maintain the underlying calculation script, plotting scripts, GUI, and documentation.\n",
        "Work with Git for version control and manage the model in GitHub.\n",
        "Develop dashboards using Streamlit (optional) and have experience with frontend/GUI development.\n",
        "Familiarity with file and data management (Pandas, JSON).\n",
        "Plotting via Matplotlib, Plotly, Seaborn.\n",
        "Data analytics skillsets in Python - PySpark (Good to have).\n",
        "Basic understanding of prompt engineering and LLMs.\n",
        "Basic exposure to Databricks.\n",
        "Database experience (SQL, like tools).\n",
        "Implement a database approach as part of the continued enhancement plan for the tools.\n",
        "\n",
        "\n",
        "Qualification\n",
        "\n",
        "\n",
        "\n",
        "Strong Python development experience with object-oriented programming.\n",
        "Experience with data modeling/calculation frameworks (e.g., numpy-style APIs, model classes).\n",
        "Familiarity with Pandas, JSON for data manipulation.\n",
        "Visualization with Matplotlib, Seaborn, or Plotly.\n",
        "Ability to interpret qualitative inputs and build features based on stakeholder feedback.\n",
        "Exposure to Databricks platform and basic PySpark usage.\n",
        "Experience with SQL and database versioning concepts.\n",
        "Hands-on experience with Git version control and collaborative codebases (GitHub).\n",
        "Strong communication and collaboration skills.\n",
        "\n",
        "\n",
        "Required Skills\n",
        "\n",
        "\n",
        "\n",
        "Communication skills (verbal, written).\n",
        "Collaborative attitude.\n",
        "Creativity.\n",
        "Abstract Thinking.\n",
        "Willing to work with unclear requirements.\n",
        "Data Analysis / Stakeholder Input Gathering.\n",
        "\n",
        "\n",
        "\n",
        "Preferred Skills\n",
        "\n",
        "\n",
        "\n",
        "Experience with Datbricks.\n",
        "Basic understanding of prompt engineering and LLMs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Interested candidates can apply by sharing their resume at techcareers@invokhr.com or apply via LinkedIn job post.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWOvoiJ5IJzl"
      },
      "source": [
        "Skill Detection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVInVv6KIGUp",
        "outputId": "767c9698-4c04-4270-dfd4-a319dd2fea13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Python Modeling',\n",
              " 'Python',\n",
              " 'Python Modeling',\n",
              " 'Python',\n",
              " 'GUI',\n",
              " 'Git',\n",
              " 'Pandas',\n",
              " 'JSON',\n",
              " 'Python',\n",
              " 'SQL',\n",
              " 'Python',\n",
              " 'SQL',\n",
              " 'Git',\n",
              " 'via',\n",
              " 'LinkedIn job post.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a function to find skills in description\n",
        "def get_skills(text):\n",
        "  #turn text into flair model format\n",
        "  sentence = Sentence(text)\n",
        "  #use AI to detect skills\n",
        "  flair_model.predict(sentence)\n",
        "  # return the skills found\n",
        "  return [entity.text for entity in sentence.get_spans(\"ner\")]\n",
        "\n",
        "get_skills(Description)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtpORGYXLvrs"
      },
      "source": [
        "Scrapper manager class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QzfdN9mAJU5A"
      },
      "outputs": [],
      "source": [
        "#create a class to manage scrapping\n",
        "class ScraperManager:\n",
        "  #setup when class starts\n",
        "  def __init__(self):\n",
        "    #flag to stop scrapping\n",
        "    self.stop_event = threading.Event()\n",
        "    #Empty table for job data\n",
        "    self.current_df = pd.DataFrame()\n",
        "    # Lock to avoid data mix-ups\n",
        "    self.lock = threading.Lock()\n",
        "\n",
        "  #reset for new scrape\n",
        "  def reset(self):\n",
        "    #clear stop flag\n",
        "    self.stop_event.clear()\n",
        "    #clear job table\n",
        "    self.cuurent_df = pd.DataFrame()\n",
        "\n",
        "  #add one job to table\n",
        "  def add_job(self, job_data):\n",
        "    #Lock to keep data safe\n",
        "    with self.lock:\n",
        "      #make job into tiny table\n",
        "      new_df = pd.DataFrame([job_data])\n",
        "      #add job to main table\n",
        "      self.current_df = pd.concat([self.current_df, new_df], ignore_index=True)\n",
        "\n",
        "#create manage Instance\n",
        "scraper_manager = ScraperManager()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSJTXusAOhMz"
      },
      "source": [
        "Save to CSV function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AE6jlGnAOeGA"
      },
      "outputs": [],
      "source": [
        "#Define function to save jobs\n",
        "\n",
        "def save_csv(df, filename = \"jobs\"):\n",
        "  try:\n",
        "    #make folder for files\n",
        "    os.makedirs(\"saved_jobs\", exist_ok=True)\n",
        "    #set default name with time stamp\n",
        "    if not filename:\n",
        "      filename = f\"jobs_{int(time.time())}\"\n",
        "    #build file path\n",
        "    full_path = f\"saved_jobs/{filename}.csv\"\n",
        "    #save table to csv\n",
        "    df.to_csv(full_path, index=False)\n",
        "    #confirm save worked\n",
        "    return f\"Saved to {full_path}\"\n",
        "  except Exception as e:\n",
        "    #show error if save fails\n",
        "    return f\"Save error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8XD8ppcT7gX"
      },
      "source": [
        "**Process Job function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-JvcxVnLpATh"
      },
      "outputs": [],
      "source": [
        "# define functions to extract job details\n",
        "def process_job(job, work_type, exp_level, position):\n",
        "    try:\n",
        "        # find job title\n",
        "        title_element = job.find(\"h3\", class_=\"base-search-card__title\")\n",
        "        # find company name\n",
        "        company_element = job.find(\"a\", class_=\"hidden-nested-link\")\n",
        "        # find job location\n",
        "        loc_element = job.find(\"span\", class_=\"job-search-card__location\")\n",
        "        # find job link\n",
        "        link_element = job.find(\"a\", class_=\"base-card__full-link\")\n",
        "\n",
        "        # check all data exists\n",
        "        if not all([title_element, company_element, loc_element, link_element]):\n",
        "            return None\n",
        "\n",
        "        # clean title text\n",
        "        title = title_element.text.strip()\n",
        "        # clean company text\n",
        "        company = company_element.text.strip()\n",
        "        # clean location text\n",
        "        loc = loc_element.text.strip()\n",
        "        # clean link (remove extra bits)\n",
        "        link = link_element[\"href\"].split(\"?\")[0]\n",
        "\n",
        "        # setup web session with retries\n",
        "        session = requests.Session()\n",
        "        # set retry rules for errors\n",
        "        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 503, 504])\n",
        "        # apply retries to session\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        # default if description fails\n",
        "        desc = \"Description not available\"\n",
        "        # empty skills list\n",
        "        skills = []\n",
        "\n",
        "        try:\n",
        "            # wait 2-5 seconds to avoid bans\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "\n",
        "            # fetch the job page\n",
        "            response = session.get(\n",
        "                link,\n",
        "                headers={\n",
        "                    # random browser to seem like human\n",
        "                    'User-Agent': random.choice([\n",
        "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "                        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
        "                        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n",
        "                    ]),\n",
        "                    # set language for consistency\n",
        "                    'Accept-Language': 'en-US,en;q=0.9'\n",
        "                },\n",
        "                # timeout after 10 seconds\n",
        "                timeout=10\n",
        "            )\n",
        "\n",
        "            # parse job page HTML\n",
        "            job_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # list of places to find description\n",
        "            description_selectors = [\n",
        "                'div.description__text',\n",
        "                'div.show-more-less-html__markup',\n",
        "                'div.core-section-container__content',\n",
        "                'section.core-section-container'\n",
        "            ]\n",
        "\n",
        "            # try each description spot\n",
        "            for selector in description_selectors:\n",
        "                desc_element = job_soup.select_one(selector)\n",
        "                if desc_element:\n",
        "                    # clean description text\n",
        "                    desc = desc_element.get_text(\"\\n\").strip()\n",
        "                    # find skills with AI\n",
        "                    skills = get_skills(desc)\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            # log error if page fails\n",
        "            print(f\"Error processing {link}: {str(e)}\")\n",
        "\n",
        "        # return job details\n",
        "        return {\n",
        "            \"Position\": position,\n",
        "            \"Date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            \"Work Type\": work_type,\n",
        "            \"Level\": exp_level,\n",
        "            \"Title\": title,\n",
        "            \"Company\": company,\n",
        "            \"Location\": loc,\n",
        "            \"Link\": f\"[{link}]({link})\",\n",
        "            \"Description\": desc,\n",
        "            \"Skills\": \", \".join(skills[:5]) if skills else \"No skills detected\"\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # log error if job card fails\n",
        "        print(f\"Error processing job card: {str(e)}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEVBDDgppKQk"
      },
      "source": [
        "**Scrapae Job function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IQHdM3bsyd3O"
      },
      "outputs": [],
      "source": [
        "# define function to scrape job listings\n",
        "def scrape_jobs(location, position, work_types, exp_levels, time_filter):\n",
        "    # setup web sessions\n",
        "    session = requests.Session()\n",
        "    # set retry rules\n",
        "    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "    # apply retries\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "    # loop through work types\n",
        "    for work_type in work_types:\n",
        "        # loop through experience levels\n",
        "        for exp_level in exp_levels:\n",
        "            # stop if user says so\n",
        "            if scraper_manager.stop_event.is_set():\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # Build LinkedIn search URL\n",
        "                base_url = (\n",
        "                    f\"https://www.linkedin.com/jobs/search/?keywords={position}&location={location}\"\n",
        "                    f\"&{work_type_mapping[work_type]}\"\n",
        "                    f\"&{experience_level_mapping[exp_level]}\"\n",
        "                    f\"&{time_filter_mapping[time_filter]}\"\n",
        "                    f\"&radius=0\"  # define radius to extract jobs only for specific cities\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    # Get search page\n",
        "                    response = session.get(base_url, timeout=10)\n",
        "                    # parse HTML\n",
        "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "                    # find total job count\n",
        "                    total_jobs = int(\n",
        "                        soup.find(\"h1\", class_=\"results-context-header__job-count\").text.replace(\",\", \"\")\n",
        "                    )\n",
        "                except:\n",
        "                    # default to 25 if count fails\n",
        "                    total_jobs = 25\n",
        "\n",
        "                # cap total_jobs to 100\n",
        "                total_jobs = min(100, total_jobs)\n",
        "\n",
        "                # Loop through pages (25 jobs each)\n",
        "                for start in range(0, total_jobs, 25):\n",
        "                    # stop if user says so\n",
        "                    if scraper_manager.stop_event.is_set():\n",
        "                        return\n",
        "                    # wait 2-5 seconds\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "\n",
        "                    # add page number to url\n",
        "                    url = f\"{base_url}&start={start}\"\n",
        "                    try:\n",
        "                        # get page\n",
        "                        response = session.get(url, timeout=10)\n",
        "                        # parse HTML\n",
        "                        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "                        # find all job cards\n",
        "                        job_cards = soup.find_all(\"div\", class_=\"base-card\")\n",
        "                    except Exception as e:\n",
        "                        # log error if page fails\n",
        "                        print(f\"Failed to scrape page {start}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                    # mix up job order\n",
        "                    random.shuffle(job_cards)\n",
        "\n",
        "                    # process each job\n",
        "                    for job in job_cards:\n",
        "                        # stop if user says so\n",
        "                        if scraper_manager.stop_event.is_set():\n",
        "                            return\n",
        "\n",
        "                        # get job details\n",
        "                        job_data = process_job(job, work_type, exp_level, position)\n",
        "                        # add job to table\n",
        "                        if job_data:\n",
        "                            scraper_manager.add_job(job_data)\n",
        "                            yield  # update app\n",
        "\n",
        "            except Exception as e:\n",
        "                # log big error\n",
        "                print(f\"Scraping error: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSRX5dFsylcM"
      },
      "source": [
        "**Run scrappre function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tWcTHq1y4UK0"
      },
      "outputs": [],
      "source": [
        "# define function to start scraping\n",
        "def run_scrapper(cities, states, positions, work_types, exp_levels, time_filter):\n",
        "    # clear old data\n",
        "    scraper_manager.reset()\n",
        "\n",
        "    # split cities into list\n",
        "    cities_list = [c.strip() for c in cities.split(',') if c.strip()]\n",
        "\n",
        "    # split states/countries into list\n",
        "    states_list = [s.strip() for s in states.split(',') if s.strip()]\n",
        "\n",
        "    # combine cities and states\n",
        "    locations = [f\"{city}, {state}\" for city in cities_list for state in states_list]\n",
        "\n",
        "    # clean and format positions\n",
        "    positions_list = [p.strip().replace(' ', '%20') for p in positions.split(',') if p.strip()]\n",
        "\n",
        "    # define scraping task\n",
        "    def worker():\n",
        "        # loop through locations\n",
        "        for loc in locations:\n",
        "            # loop through positions\n",
        "            for pos in positions_list:\n",
        "                # stop if user says so\n",
        "                if scraper_manager.stop_event.is_set():\n",
        "                    return\n",
        "                # run scrape_jobs\n",
        "                for _ in scrape_jobs(loc, pos, work_types, exp_levels, time_filter):\n",
        "                    pass  # scrape_jobs yields after each job is added\n",
        "\n",
        "    # start task in background\n",
        "    thread = threading.Thread(target=worker)\n",
        "    thread.start()\n",
        "\n",
        "    # update app while running\n",
        "    while thread.is_alive():\n",
        "        # wait a bit\n",
        "        time.sleep(0.5)\n",
        "        # lock data\n",
        "        with scraper_manager.lock:\n",
        "            # show progress and table\n",
        "            yield 'Scraping in progress...', scraper_manager.current_df\n",
        "\n",
        "    # show final status\n",
        "    yield (\n",
        "        \"Scraping complete!\" if not scraper_manager.stop_event.is_set() else \"Scraping stopped.\",\n",
        "        scraper_manager.current_df\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWhn-ySy4lZl"
      },
      "source": [
        "Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "c5bwmgSoBF32",
        "outputId": "12503332-3628-48da-a3f5-481bb0857758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6116cc44e97e933c68.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6116cc44e97e933c68.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build the Web app\n",
        "with gr.Blocks() as app:\n",
        "    # Add title\n",
        "    gr.Markdown(\"\"\"\n",
        "    <div style=\"text-align: center; color: #f67d3c; font-size: 2em; font-weight:bold; margin:20px 0; padding: 10px\">\n",
        "      AI-Powered LinkedIn Job Scraper\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # create user input section\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # input for the cities\n",
        "            cities = gr.Textbox(label=\"Cities (comma-separated)\")\n",
        "            # Input for States\n",
        "            states = gr.Textbox(label=\"States/Countries (comma-separated)\")\n",
        "            # input for jobs\n",
        "            positions = gr.Textbox(label=\"Positions (comma-separated)\")\n",
        "            # checkbox for work type\n",
        "            work_types = gr.CheckboxGroup(list(work_type_mapping.keys()), label=\"Work Types\")\n",
        "            # checkbox for experience level\n",
        "            exp_levels = gr.CheckboxGroup(list(experience_level_mapping.keys()), label=\"Experience Levels\")\n",
        "            # dropdown for time\n",
        "            time_filter = gr.Dropdown(list(time_filter_mapping.keys()), label=\"Time Filter\")\n",
        "\n",
        "      # Buttons for start/stop\n",
        "    with gr.Row():\n",
        "      start_btn = gr.Button(\"Start Scraping\", variant=\"primary\")\n",
        "      stop_btn = gr.Button(\"Stop Scraping\", variant=\"secondary\")\n",
        "\n",
        "    # show status\n",
        "    status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    # Show job table\n",
        "    results = gr.Dataframe(\n",
        "        headers=[\"Position\", \"Date\", \"Work Type\", \"Level\", \"Title\", \"Company\", \"Location\", \"Link\", \"Skills\"],\n",
        "        datatype=[\"str\", \"str\", \"str\", \"str\", \"str\", \"str\", \"str\", \"str\", \"str\"],\n",
        "        interactive=False\n",
        "        )\n",
        "\n",
        "    # save section\n",
        "    with gr.Row():\n",
        "        # Input for file name\n",
        "        filename = gr.Textbox(label=\"Filename (Optional)\", placeholder=\"my_jobs\")\n",
        "        # save button\n",
        "        save_btn = gr.Button(\"Save to CSV\", variant=\"secondary\")\n",
        "\n",
        "    # show save status\n",
        "    save_status = gr.Textbox(label=\"Save Status\")\n",
        "\n",
        "    # Prep start: clear any previous stop signal\n",
        "    start_btn.click(\n",
        "        lambda: scraper_manager.stop_event.clear(),\n",
        "        outputs=None\n",
        "    )\n",
        "\n",
        "    # Link start button to scraper (streaming updates)\n",
        "    start_btn.click(\n",
        "        run_scrapper,\n",
        "        inputs=[cities, states, positions, work_types, exp_levels, time_filter],\n",
        "        outputs=[status, results]\n",
        "    )\n",
        "\n",
        "    # link stop button to pause\n",
        "    stop_btn.click(\n",
        "        lambda: scraper_manager.stop_event.set(),\n",
        "        outputs=None\n",
        "    )\n",
        "\n",
        "    # link save button to save\n",
        "    save_btn.click(\n",
        "        save_csv,\n",
        "        inputs=[results, filename],\n",
        "        outputs=save_status\n",
        "    )\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17259ae004d443e8b21c4bc0ce5e2e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d007aedc45a149b889545f1b47d8208f",
              "IPY_MODEL_4df204eac5e4432a956a673f5100262f",
              "IPY_MODEL_94ac2c7c1a5e41f6a78ebf4296d65cd9"
            ],
            "layout": "IPY_MODEL_e149aab73f9f40ea8ef7553e43838f02"
          }
        },
        "2a4ebb5d94ba423e81c7f139a357972b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4df204eac5e4432a956a673f5100262f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af16c9524c324050bab76847cac365e3",
            "max": 169020883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df19849846ea44b8a5da8392c6708b14",
            "value": 169020883
          }
        },
        "7e01df3e79ee4b828774f7f897cddc88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ac2c7c1a5e41f6a78ebf4296d65cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe7ccd4facd94b85becf3f55303a0d1a",
            "placeholder": "​",
            "style": "IPY_MODEL_bf930c21b2314401b10ef686b6cb7334",
            "value": " 169M/169M [00:01&lt;00:00, 135MB/s]"
          }
        },
        "af16c9524c324050bab76847cac365e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf930c21b2314401b10ef686b6cb7334": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d007aedc45a149b889545f1b47d8208f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e01df3e79ee4b828774f7f897cddc88",
            "placeholder": "​",
            "style": "IPY_MODEL_2a4ebb5d94ba423e81c7f139a357972b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "df19849846ea44b8a5da8392c6708b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e149aab73f9f40ea8ef7553e43838f02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe7ccd4facd94b85becf3f55303a0d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
